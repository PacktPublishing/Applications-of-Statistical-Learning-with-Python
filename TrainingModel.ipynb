{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning to Feel; Training the Model\n",
    "*Curtis Miller*\n",
    "\n",
    "Now that we have data, let's attempt to train a model for predicting emotions from images. Let's first load in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "faces = pd.read_csv(\"fer2013.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's split into training and testing data. We will use the suggested categories provided with the dataset, and use the private test set rather than the public test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "facegroups = faces.groupby(\"Usage\")\n",
    "face_train = facegroups.get_group(\"Training\")\n",
    "face_test = facegroups.get_group(\"PrivateTest\")\n",
    "face_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is code from the previous video."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emotion_code = {0: \"angry\",\n",
    "                1: \"disgust\",\n",
    "                2: \"fear\",\n",
    "                3: \"happy\",\n",
    "                4: \"sad\",\n",
    "                5: \"surprise\",\n",
    "                6: \"neutral\"}\n",
    "\n",
    "def string_to_image(pixelstring):\n",
    "    return np.array(pixelstring.split(' '), dtype=np.int16).reshape(48, 48)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For emotion detection we will use the LBPH recognizer that is supplied with OpenCV. Our classes are different emotions, and the classifier will learn to group and separate faces based on these classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emotion_detector = cv2.face.LBPHFaceRecognizer_create()\n",
    "emotion_detector.train(face_train.pixels.map(string_to_image), face_train.emotion.as_matrix())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How does this classifier do on the training set?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(string_to_image(face_train.pixels[8]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emotion_detector.predict_label(string_to_image(face_train.pixels[8]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It handles one face well. Let's see how it does on training data. We won't use the entire dataset simply because it would take too long on my laptop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estimated to take 10 minutes on my laptop\n",
    "train_emotion_predicted = face_train.pixels[:100].map(lambda x: emotion_detector.predict_label(string_to_image(x)))\n",
    "train_emotion_predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(face_train.emotion[:100], train_emotion_predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This report is too good to be true. It's not surprising considering that classification using LBPH shares a lot in common with nearest neighbor approaches; it will do perfectly on the training set as a result.\n",
    "\n",
    "If we wanted a better sense of how well the algorithm would do, we should employ cross-validation. However, that would be too time consuming for this video.\n",
    "\n",
    "So how does the algorithm do on test data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_emotion_predicted = face_test.pixels[:100].map(lambda x: emotion_detector.predict_label(string_to_image(x)))\n",
    "print(classification_report(face_test.emotion[:100], test_emotion_predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(accuracy_score(face_test.emotion[:100], test_emotion_predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These results don't appear particularly encouraging. However, emotion recognition is a hard problem (and perhaps defined inappropriately). Our algorithm does do better than random guessing or simply guessing the most common label (the latter case would yield an accuracy of 22%). Since there's \"overlap\" between emotions, it's also possible that the algorithm guesses similar (but \"incorrect\") emotions.\n",
    "\n",
    "So we may not have done too poorly.\n",
    "\n",
    "(In the [Kaggle competition](https://www.kaggle.com/c/challenges-in-representation-learning-facial-expression-recognition-challenge/leaderboard) the best accuracy was 71%; our algorithm, if it maintained this accuracy, would place 31st out of 56, and we haven't done that much.)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
