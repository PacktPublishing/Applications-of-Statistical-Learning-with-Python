{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning How to Read: Text Preprocessing\n",
    "\n",
    "A big part of working with natural languages is preparing a text for later work. In this notebook I demonstrate the process of preparing a document using an e-mail sent out by [arXiv.org](https://arxiv.org/), a website containing preprints of academic papers in the sciences. This particular e-mail contains the names and abstracts of recent papers from select computer science subjects.\n",
    "\n",
    "I load in the e-mail as a string below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(\"arxiv_email_cs_030918.txt\") as f:\n",
    "    email_string = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(email_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "email_string[:(1024 * 10)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In raw form there is a lot of undesired formatting. We may be primarily interested in abstracts, but we will need to identify their locations. Additionally there are newline characters, `\\n`, placed in an undesirable way; they separate lines, not necessarily paragraphs. We will see how to deal with these issues, along with how to approach the words in the e-mail.\n",
    "\n",
    "## Noise and Formatting\n",
    "\n",
    "This e-mail contains multiple abstracts, and we may view each one as its own document. In addition to the abstract, we may think that the title and authors are interesting information. Formatting also separates one abstract from another. But all other formatting and information should be removed, and the abstracts should not have lines separated as they are.\n",
    "\n",
    "We would like to turn this into a corpus of documents, each one containing a single abstract with appropriate formatting, and also track title and authorship information in a separate document. A lot of the work we want to do can be done using regular expressions.\n",
    "\n",
    "In this e-mail, not all papers have abstracts (they could be revisions to existing papers). Those papers are listed at the bottom of the document and should be excluded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re, string\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abstract = r\"\"\"-{78}[\\n]                        # A line of ------\n",
    "               [\\\\]{2}[\\n]                      # Two slashes, new line\n",
    "               arXiv:[0-9.]+[\\n]                # arXiv number\n",
    "               Date:.*                          # Has a date\n",
    "               [\\n]{2}                          # Two new lines\n",
    "               Title:.*[\\n](?:.*[\\n])*?         # Capture Title field, including multiple lines\n",
    "               Authors:.*[\\n](?:.*[\\n])*?       # Capture Author field, ...\n",
    "               Categories:.*[\\n](?:.*[\\n])*?    # Capture Categories field, ...\n",
    "               (?:Comments:.*[\\n](?:.*[\\n])*?)? # If a Comments field exists, capture it too, ...\n",
    "               [\\\\]{2}[\\n]                      # Check for an isolated \\\\ ; this helps catch articles w/ abstracts\n",
    "               \\s.*[\\n](?:.*[\\n])*?             # Abstracts starts with a space; then get the rest of the content\n",
    "               [\\\\]{2}.*[\\n]                    # Line ends with \\\\ ( ... ) so get this\n",
    "               -{78}                            # Final line of -------\n",
    "            \"\"\"\n",
    "\n",
    "abstract_strs = re.findall(abstract, email_string, re.X)    # Using re.X allows up to split up our regex and add comments\n",
    "print(abstract_strs[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(abstract_strs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`abstract_strs` contains abstract substrings; now we want to extract abstracts and other data. Our earlier regular expression can be modified to extract this data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abstract_title = r\"\"\"Title: (.*(?:.*[\\n])*?)Authors\"\"\"\n",
    "\n",
    "abstract_title_strs = list(map(lambda x: re.findall(abstract_title, x)[0][:-1], abstract_strs))    # [:-1] to remove \\n\n",
    "abstract_title_strs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abstract_authors = r\"\"\"Authors: (.*(?:.*[\\n])*?)Categories\"\"\"\n",
    "\n",
    "abstract_authors_strs = list(map(lambda x: re.findall(abstract_authors, x)[0][:-1], abstract_strs))\n",
    "abstract_authors_strs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abstract_text = r\"\"\"[\\\\][\\n]\\s(.*(?:.*[\\n])*?)[\\\\].*[\\n]-\"\"\"\n",
    "\n",
    "abstract_text_strs = list(map(lambda x: re.findall(abstract_text, x)[0][1:-1], abstract_strs))\n",
    "abstract_text_strs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we replace all whitespace characters with a single space for every entry of these lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abstract_title_strs = list(map(lambda x: re.sub(r\"\\s+\", \" \", x), abstract_title_strs))\n",
    "abstract_authors_strs = list(map(lambda x: re.sub(r\"\\s+\", \" \", x), abstract_authors_strs))\n",
    "abstract_text_strs = list(map(lambda x: re.sub(r\"\\s+\", \" \", x), abstract_text_strs))\n",
    "\n",
    "abstract_title_strs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abstract_authors_strs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abstract_text_strs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the case of authors we may actually want a consistent formatting (notice that sometimes the names are separated by `,` and sometimes by \"and\"). Let's address that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abstract_authors_strs = list(map(lambda x: re.sub(r\",\", \" and\", x), abstract_authors_strs))\n",
    "abstract_authors_strs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization\n",
    "\n",
    "Tokenization separates a sentence into tokens, which are words, parts of words (for example, we may separate `it's` into `it` and `'s`), or punctuation. The na√Øve approach is to split on spaces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abstract_text_strs[0].split(' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLTK provides smarter tokenizers for us to use, though. There are several options to choose from, but we'll keep it simple and use `wordpunct_tokenize()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import wordpunct_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(wordpunct_tokenize(abstract_text_strs[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abstract_text_structs = list(map(wordpunct_tokenize, abstract_text_strs))\n",
    "abstract_text_structs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part of Speech Tagging\n",
    "\n",
    "The next step I will take is to tag the words (or rather their stems) with part of speech tags, which label the words in the sentence with their part of speech classification (for example, \"book\" is a noun, \"he\" is an adverb, etc.).\n",
    "\n",
    "The recommended part of speech tagger from NLTK is `pos_tag()`, though the package offers many taggers and facilities for training a tagger.\n",
    "\n",
    "For each word, the tagger creates a tuple, with the first string in the tuple being the word, and the second being the word's part of speach classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.tag import pos_tag, pos_tag_sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_tag(abstract_text_structs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.help.upenn_tagset()    # See what tags mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "abstract_text_structs = pos_tag_sents(abstract_text_structs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming\n",
    "\n",
    "Are the words \"run\" and \"running\" the same? If we think so, we may want to use a stemmer to extract \"run\" from both words, as \"run\" is the stem of the word of interest in both cases.\n",
    "\n",
    "NLTK provides stemmers, one of which being the popular Snowball stemmer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.stem import SnowballStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = SnowballStemmer(\"english\")\n",
    "stemmer.stem(\"running\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = abstract_text_structs[0]\n",
    "abstract_text_structs = [[(stemmer.stem(w[0]), w[1]) for w in a] for a in abstract_text_structs]\n",
    "abstract_text_structs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing Stopwords and Punctuation\n",
    "\n",
    "Words like \"and\", \"the\", \"a\", etc. don't distinguish documents, so we want to remove them. Also we are not particularly interested in punctuation, so we will remove that as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords.words(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abstract_text_structs = [[(w[0], w[1]) for w in a if w[0] not in stopwords.words(\"english\") and \\\n",
    "                          w[0] not in string.punctuation] for a in abstract_text_structs]\n",
    "abstract_text_structs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will save our work in a collection of files organized as a tagged corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "ids = pd.DataFrame({\"title\": abstract_title_strs, \"authors\": abstract_authors_strs},\n",
    "                   index=pd.Index([\"abs\" + str(i) for i in range(len(abstract_text_strs))]))\n",
    "ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids.to_csv(\"abstracts_id.csv\")\n",
    "\n",
    "' '.join(w[0] + \"/\" + w[1] for w in abstract_text_structs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "os.makedirs(\"abstracts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for a, name in zip(abstract_text_structs, ids.index):\n",
    "    abstract_file_text = ' '.join(w[0] + \"/\" + w[1] for w in a)\n",
    "    with open(\"abstracts/\" + name + \".txt\", mode='x') as f:\n",
    "        f.write(abstract_file_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now handle these files as a tagged NLTK corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus.reader.tagged import TaggedCorpusReader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "abstracts_dir = os.path.abspath('abstracts')\n",
    "abstract_corpus = TaggedCorpusReader(abstracts_dir, \".*\\.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abstract1 = nltk.Text(abstract_corpus.words(\"abs0.txt\"))\n",
    "abstract1[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abstract1.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abstract1.collocations()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abstract_corpus.tagged_words()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abstract_corpus.tagged_words(fileids=[\"abs10.txt\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have a (small) dataset ready for later work."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
