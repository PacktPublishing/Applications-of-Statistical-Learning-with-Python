{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using $n$-Grams and Bag-of-Words Models\n",
    "*Curtis Miller*\n",
    "\n",
    "In this video I demonstrate how to use $n$-grams and bag-of-words models. I show not only how to create the relevant data structures from documents, I also give usage examples.\n",
    "\n",
    "## $n$-Grams\n",
    "\n",
    "$n$-grams can either refer to either collections of characters or words. I will to the words case later; for now, let's focus on character $n$-grams.\n",
    "\n",
    "A $n$-gram is a sequence of $n$ characters that appear in a text. The 3-grams for the word \"apple\" are `[\"app\", \"ppl\", \"ple\"]`, and the 4-grams are `[\"appl\", \"pple\"]`. $n$-grams are used to generate a feature set for a string that we can use for other purposes, such as machine learning.\n",
    "\n",
    "Let's demonstrate using $n$-grams to identify gender in names. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk import ngrams\n",
    "from nltk.corpus import names\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.naive_bayes import BernoulliNB, GaussianNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names.fileids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names.words(fileids=\"female.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "femdf = pd.DataFrame({\"name\": names.words(fileids=\"female.txt\"),\n",
    "                      \"gender\": [\"female\"] * len(names.words(fileids=\"female.txt\"))})\n",
    "femdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "maldf = pd.DataFrame({\"name\": names.words(fileids=\"male.txt\"),\n",
    "                      \"gender\": [\"male\"] * len(names.words(fileids=\"male.txt\"))})\n",
    "maldf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "namedf = maldf.append(femdf)\n",
    "namedf.index = pd.Index([u for u in range(namedf.shape[0])])\n",
    "namedf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nametrain, nametest = train_test_split(namedf)\n",
    "nametrain.index = pd.Index([u for u in range(nametrain.shape[0])])\n",
    "nametest.index = pd.Index([u for u in range(nametest.shape[0])])\n",
    "nametrain.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nametrain.gender.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nametest.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "namegrams = [[''.join(u) for u in ngrams(m, n=3)] for m in nametrain.name]\n",
    "namegrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gramfreq = nltk.FreqDist(list(gr for a in namegrams for gr in a))\n",
    "gramfreq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gramfreq.plot(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M = 2000\n",
    "gramfreq.most_common(M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "commongrams = [gr[0] for gr in gramfreq.most_common(M)]\n",
    "gramdf = pd.DataFrame(np.zeros((nametrain.shape[0], M)),\n",
    "                      columns=pd.Index(commongrams))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gramdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nametrain = nametrain.join(gramdf)\n",
    "\n",
    "for i in nametrain.index:\n",
    "    nametrain.loc[nametrain.index[i], list(u for u in namegrams[i] if u in commongrams)] = 1\n",
    "\n",
    "nametrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gendpred = BernoulliNB()\n",
    "\n",
    "gendpred = gendpred.fit(nametrain.loc[:, commongrams], nametrain.gender)\n",
    "predicted_gender = pd.Series(gendpred.predict(nametrain.as_matrix(commongrams)))\n",
    "predicted_gender.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(nametrain.gender, predicted_gender))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nametest = nametest.join(gramdf)\n",
    "namegramstest = [[''.join(u) for u in ngrams(m, n=3)] for m in nametest.name]\n",
    "\n",
    "for i in range(nametest.shape[0]):\n",
    "    nametest.loc[nametest.index[i], list(u for u in namegramstest[i] if u in commongrams)] = 1\n",
    "\n",
    "predicted_gender_test = pd.Series(gendpred.predict(nametest.as_matrix(commongrams)))\n",
    "print(classification_report(nametest.gender, predicted_gender_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The classifier does not do a terrible job, but I'm mostly interested in demonstrating the technology at this point.\n",
    "\n",
    "## Bag-of-Words\n",
    "\n",
    "The idea of bag-of-words models is essentially the same as $n$-grams when applied to characters, though now we work with words. We again can combine words to make bigrams, trigrams, etc., which may be more useful.\n",
    "\n",
    "Ultimately these methods are a form of feature generation for documents that can later be used for learning applications.\n",
    "\n",
    "Here, I use bigrams to predict whether a speech (specifically, an American State of the Union address or inaugural address) was delivered by a Democratic or Republican president. I use the State of the Union and inaugural address corpora provided with NLTK for training, and will use the na√Øve Bayes algorithm for classification. The number of times a bigram appears in a speech will be a feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import state_union, inaugural"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_union.fileids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inaugural.fileids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_union.words('2006-GWBush.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will use only speeches given during the \"modern\" American political era, which I consider to start with President Eisenhower."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "state_union.fileids()[7:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inaugural.fileids()[41:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pres_party = {            # Will be used to identify parties\n",
    "    \"Eisenhower\": \"R\",\n",
    "    \"Kennedy\": \"D\",\n",
    "    \"Johnson\": \"D\",\n",
    "    \"Nixon\": \"R\",\n",
    "    \"Ford\": \"R\",\n",
    "    \"Carter\": \"D\",\n",
    "    \"Reagan\": \"R\",\n",
    "    \"Bush\": \"R\",\n",
    "    \"Clinton\": \"D\",\n",
    "    \"Obama\": \"D\",\n",
    "    \"GWBush\": \"R\",\n",
    "    \"Trump\": \"R\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.Text(state_union.words(\"2002-GWBush.txt\")).collocations()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataframe containing speech data; for now, this is file id, type of speech, and party\n",
    "speeches = pd.DataFrame({\"fileid\": [*state_union.fileids()[7:], *inaugural.fileids()[41:]],\n",
    "                         \"type\": [*([\"sotu\"] * len(state_union.fileids()[7:])), \n",
    "                                  *([\"inaugural\"] * len(inaugural.fileids()[41:]))]})\n",
    "speeches = speeches.join(pd.DataFrame({\"party\": speeches.fileid.map(\n",
    "    lambda x: pres_party[re.findall(\"[0-9]{4}-(.*?)(?:-[0-9])?\\.txt\", x)[0]])}))\n",
    "speeches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get a collection of tokens for Democratic and Republican speeches\n",
    "tokens_R = list()\n",
    "tokens_D = list()\n",
    "\n",
    "for _, s in speeches.iterrows():\n",
    "    if s.type == \"sotu\":\n",
    "        words = state_union.words(fileids=s.fileid)\n",
    "    else:\n",
    "        words = inaugural.words(fileids=s.fileid)\n",
    "    \n",
    "    if s.party == \"R\":\n",
    "        tokens_R.extend(list(words))\n",
    "    else:\n",
    "        tokens_D.extend(list(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_R"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I don't want to use every single possible bigram as my feature space. Instead I will find two-word collocations, words that appear unusually often in a text. I find collocations for both Democratic and Republican presidents, then combine the collocations into one common set that will be treated as the feature space. Below are some functions for finding these collocations, using functionality provided by NLTK (along with some NLTK source code)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.collocations import BigramAssocMeasures, BigramCollocationFinder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_collocations(l, window_size=2, num=20):\n",
    "    \"\"\"Gets a list of collocations for a text; this is similar to code from the collocations() method of Text in nltk\"\"\"\n",
    "    \n",
    "    ignored_words = stopwords.words('english')\n",
    "    finder = BigramCollocationFinder.from_words(l, window_size)\n",
    "    finder.apply_freq_filter(2)\n",
    "    finder.apply_word_filter(lambda w: len(w) < 3 or w.lower() in ignored_words or w == w.upper())\n",
    "    bigram_measures = BigramAssocMeasures()\n",
    "    return finder.nbest(bigram_measures.likelihood_ratio, num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colloc_R = get_collocations(tokens_R, window_size=2, num=60)\n",
    "colloc_R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colloc_D = get_collocations(tokens_D, window_size=2, num=60)\n",
    "colloc_D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the common collocation set\n",
    "feature_colloc = list(set(' '.join(w) for w in colloc_R).union(' '.join(w) for w in colloc_D))\n",
    "feature_colloc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I find bigrams for the speeches and compute how frequently a word pair appeared in a speech, for each speech. The frequency of these word pairs will be treated as the data points that will form the basis of prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phrase_dict = dict()\n",
    "\n",
    "for i, s in speeches.iterrows():\n",
    "    if s.type == \"sotu\":\n",
    "        words = state_union.words(fileids=s.fileid)\n",
    "    else:\n",
    "        words = inaugural.words(fileids=s.fileid)\n",
    "    \n",
    "    bigrams = [u[0] + ' ' + u[1] for u in nltk.ngrams(words, 2)]\n",
    "    phrase_dict[i] = pd.Series(map(lambda x: bigrams.count(x), feature_colloc), index=feature_colloc)\n",
    "\n",
    "phrase_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "speeches = speeches.join(pd.DataFrame(phrase_dict).T)\n",
    "speeches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we fit a na√Øve Bayes classifier to the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "partypred = GaussianNB()\n",
    "\n",
    "partypred = partypred.fit(speeches.loc[:, feature_colloc], speeches.party)\n",
    "\n",
    "party_predicted = partypred.predict(speeches.loc[:, feature_colloc])\n",
    "party_predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(speeches.party, party_predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classification isn't bad, though not great either. Now let's test out the classifier on unseen data: Barack Obama's 2014 State of the Union address and Donald Trump's 2018 State of the Union address."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"2014-Obama.txt\") as f:\n",
    "    obama_speech = f.read()\n",
    "\n",
    "with open(\"2018-Trump.txt\") as f:\n",
    "    trump_speech = f.read()\n",
    "\n",
    "print(obama_speech)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(trump_speech)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "token_obama = nltk.tokenize.wordpunct_tokenize(obama_speech)\n",
    "token_trump = nltk.tokenize.wordpunct_tokenize(trump_speech)\n",
    "\n",
    "obama_bigrams = [u[0] + ' ' + u[1] for u in nltk.ngrams(token_obama, 2)]\n",
    "trump_bigrams = [u[0] + ' ' + u[1] for u in nltk.ngrams(token_trump, 2)]\n",
    "\n",
    "test_data = pd.DataFrame({\"obama\": pd.Series(map(lambda x: obama_bigrams.count(x), feature_colloc),\n",
    "                                             index=feature_colloc),\n",
    "                          \"trump\": pd.Series(map(lambda x: trump_bigrams.count(x), feature_colloc),\n",
    "                                             index=feature_colloc)}).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "partypred.predict(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately our classifier did not do well on the test set; it only has an accuracy of 50%, identifying Donald Trump as a Democrat."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
